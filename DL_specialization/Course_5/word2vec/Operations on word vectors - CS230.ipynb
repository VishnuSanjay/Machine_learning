{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on word vectors\n",
    "\n",
    "Welcome to your second programming assignment of week 1. You are going to apply operations between word vectors in order to assess their workability and usefulness. In the last part, you are going to remove the bias hurting these word vectors. \n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "- Apply word vectors to operations like “king” - “queen”\n",
    "- Code and understand cosine similarity\n",
    "- Understand word analogies\n",
    "- Encode concepts such as \"gender\" in a vector\n",
    "- Debias word vectors using a series of projections\n",
    "\n",
    "Let's get started! Run the following cell to load the packages your are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from w2v_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also load the word vectors from an embedding matrix that maps every english word (from a wide vocabulary) into a vector. In this notebook, we chose to use 50-dimensional GloVe vectors to represent our words. Run the following cell to load the `word_to_vec_map` which contains all the vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've loaded:\n",
    "- `words`: list of all the words from the vocabulary.\n",
    "- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation.\n",
    "\n",
    "You've seen that one-hot vectors are not comparable in term of word meaning. Now you have word representations and the question is: how do you define similarity between two word vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Cosine similarity\n",
    "\n",
    "We need a way to find the similarity between vectors and this can be done using Cosine Similarity. Mathematically, for two vectors $u$ and $v$,\n",
    "\n",
    "$$\\text{CosineSimilarity(u, v)} = \\frac {u . v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$\n",
    "\n",
    "where $u.v$ is the dot (or scalar) product of two vectors, $||u||_2$ is the magnitude of the vector $u$, $\\theta$ is the angle between $u$ and $v$. This similarity depends actually on the angle between $u$ and $v$ as you can see on the figure below.\n",
    "\n",
    "<img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
    "<caption><center> **Figure 1**: The cosine of the angle between word vectors is a good metric to define their similarity</center></caption>\n",
    "\n",
    "**Exercise**: Implement the function `cosine_similarity()` to evaluate similarity between word vectors.\n",
    "\n",
    "**Reminder**: The magnitude of $u$ is defined by $ ||u||_2 = \\sqrt{\\sum_{i=0}^{n} u_i^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: cosine_similarity\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Similarity metric defined by the formula above\n",
    "    \n",
    "    Arguments:\n",
    "    u -- a word vector of shape (n,)\n",
    "    v -- a word vector of shape (n,)\n",
    "    \n",
    "    Returns:\n",
    "    cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 0.0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "father = word_to_vec_map[\"father\"]\n",
    "mother = word_to_vec_map[\"mother\"]\n",
    "ball = word_to_vec_map[\"ball\"]\n",
    "crocodile = word_to_vec_map[\"crocodile\"]\n",
    "france = word_to_vec_map[\"france\"]\n",
    "italy = word_to_vec_map[\"italy\"]\n",
    "paris = word_to_vec_map[\"paris\"]\n",
    "rome = word_to_vec_map[\"rome\"]\n",
    "\n",
    "print(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\n",
    "print(\"cosine_similarity(france - paris, rome - italy) = \",cosine_similarity(france - paris, rome - italy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(father, mother)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.890903844289\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(ball, crocodile)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.274392462614\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(france - paris, rome - italy)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         -0.675147930817\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you got the correct expected output, please don't hesitate to modify the expected output cell above to test the cosine similarity with your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Word analogy task\n",
    "\n",
    "In the word analogy task, we complete the sentence <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. An example is <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. Mathematically, we are trying to find a word *d*, such that the associated word vectors $v_a, v_b, v_c, v_d$ are related in the following manner: $v_b - v_a$ is most similar to $v_d - v_c$. This can also be written as finding the word *d* such that $v_d$ is most similar to the \"combined vector\" $v_b - v_a + v_c$.\n",
    "\n",
    "**Exercise**: Complete the code below to be able to perform word analogies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: complete_analogy\n",
    "\n",
    "def complete_analogy(w1, w2, w3):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task as explained above.\n",
    "    \n",
    "    Arguments:\n",
    "    w1 -- a word, string\n",
    "    w2 -- a word, string\n",
    "    w3 -- a word, string\n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that best_word is most similar to  w2 - w1 + w3\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert words to lower case\n",
    "    w1, w2, w3 = w1.lower(), w2.lower(), w3.lower()\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 1 line)\n",
    "    # Compute the combined vector\n",
    "    combined_vector = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    max_cosine_sim = -1                # Initialize max_cosine_sim to the minimum possible cosine similarity\n",
    "    best_word = None                   # Initialize best_word, it will help keep track of the word to output\n",
    "\n",
    "    # loop over the whole word vector set\n",
    "    for w in words:\n",
    "        \n",
    "        # to avoid best_word being w1, w2 or w3, pass them.\n",
    "        if w in [w1, w2, w3] :\n",
    "            continue\n",
    "        \n",
    "        ### START CODE HERE ### (approx. 4 lines)\n",
    "        # Compute cosine similarity between the combined_vector and the current word\n",
    "        cosine_sim = None\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "        # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word\n",
    "        None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your code, this may take 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('large', 'larger', 'small')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **italy -> italian** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         spain -> spanish\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **india -> delhi** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         japan -> tokyo\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **man -> woman ** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         boy -> girl\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **large -> larger ** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         small -> smaller\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you got the correct expected output, please don't hesitate to modify the expected output cell above to test your own analogies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Debiasing word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise, you will look at the implications of using a particular training dataset. You will first compute a vector $v_1 - v_2$, where $v_1$ represents the word vector corresponding to the word *woman*, whereas $v_2$ corresponds to the word vector corresponding to the word *man*. The resulting vector encodes the concept of \"gender\".\n",
    "\n",
    "The code below encodes the meaning of \"gender\" in a vector by taking the difference between word vectors of \"woman\" and \"man\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gender = word_to_vec_map['woman'] - word_to_vec_map['man']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will consider the cosine similarity of different words with the constructed *gender* vector. Consider what a positive value of similarity means vs a negative cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print ('List of names and their similarities with constructed vector:')\n",
    "\n",
    "# girls and boys name\n",
    "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
    "\n",
    "for w in name_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, female first names have a positive cosine similarity with our constructed *gender* vector while male first names have a negative cosine similarity. This is not suprising and it is not a bias. Let's try with other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Other words and their similarities:')\n",
    "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
    "for w in word_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice anything surprising? It is astonishing how these results underline the real-life existing bias between women and men. For example, \"computer\" is closer to \"man\" while \"literature\" is closer to \"woman\".\n",
    "\n",
    "The dataset you choose to train your word vectors on has immense power, so you should be careful when you train! You will now remove the gender bias of some of these words.\n",
    "\n",
    "Note that some words such as \"actor\"/\"actress\" or \"grandmother\"/\"grandfather\", should remain gender specific while other words such as \"receptionist\" or \"scientist\" should be neutralized, i.e. not be gender-related.\n",
    "\n",
    "You have to treat these two type of words differently when debiasing.\n",
    "\n",
    "### 3.1 - Neutralize bias for non-gender specific words\n",
    "\n",
    "The following figure should help you visualize what neutralizing does.\n",
    "\n",
    "<img src=\"images/neutralize_kiank.png\" style=\"width:800px;height:300px;\">\n",
    "<caption><center> **Figure 2**: The word vector for \"receptionist\" represented before and after applying the neutralize operation. </center></caption>\n",
    "\n",
    "**Exercise**: Implement `neutralize()` to remove the bias of words such as \"receptionist\" or \"scientist\".\n",
    "\n",
    "**Reminder**: a vector $u$ can be split into two parts: its projection over a vector-axis $v_B$ and its projection over the axis orthogonal to $v$:\n",
    "$$u = u_B + u_{\\perp}$$\n",
    "where : $u_B = \\frac{u . v_B}{||v_B||_2 ||v_B||_2} * v_B$ and $ u_{\\perp} = u - u_B $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neutralize(word, bias_axis):\n",
    "    \"\"\"\n",
    "    Removes the bias of \"word\" by projecting it on the space orthogonal to the bias axis. \n",
    "    This function ensures that gender neutral words are zero in the gender subspace.\n",
    "    \n",
    "    Arguments:\n",
    "    word -- string indicating the word to debias\n",
    "    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word = \"receptionist\"\n",
    "bias = gender\n",
    "print(\"cosine similarity between \" + word + \" and gender, before neutralizing: \", cosine_similarity(word_to_vec_map[\"receptionist\"], gender))\n",
    "\n",
    "v = neutralize(\"receptionist\", gender)\n",
    "print(\"cosine similarity between \" + word + \" and gender, after neutralizing: \", cosine_similarity(v, gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cosine similarity between receptionist and gender, before neutralizing:** :\n",
    "        </td>\n",
    "        <td>\n",
    "         0.330779417506\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine similarity between receptionist and gender, after neutralizing:** :\n",
    "        </td>\n",
    "        <td>\n",
    "         -3.26732746085e-17\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Equalize bias for gender-specific words\n",
    "\n",
    "Now, you will debias gender specific words using a technique called equalization.\n",
    "\n",
    "Equalization is applied to pairs of words which should differ in meaning only because of their gender properties. For example, \"businessman\" and \"businesswoman\" should have the same vector representation in the space orthogonal to the *gender* space. They should differ only in the *gender* space.\n",
    "\n",
    "Equalizing can be carried out in 6 steps as explained in the figure below.\n",
    "\n",
    "<img src=\"images/equalize_kiank1.png\" style=\"width:800px;height:300px;\"> <br>\n",
    "<img src=\"images/equalize_kiank2.png\" style=\"width:800px;height:300px;\"> <br>\n",
    "<img src=\"images/equalize_kiank3.png\" style=\"width:800px;height:300px;\">\n",
    "<caption><center> **Figure 3**: The 6 steps to carry out equalizing in order to debias gender-specific words. </center></caption>\n",
    "\n",
    "**Exercise**: Implement equalize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def equalize(pair, bias_axis):\n",
    "    \"\"\"\n",
    "    Debias gender specific words by following the equalize method described in the figure above.\n",
    "    \n",
    "    Arguments:\n",
    "    pair -- pair of strings of gender specific words to debias, e.g. (\"actress\", \"actor\") \n",
    "    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Select word vector representation of \"word\". Use word_to_vec_map. (≈ 2 lines)\n",
    "    w1, w2 = None\n",
    "    u1, u2 = None\n",
    "    \n",
    "    # Step 2: Compute the mean of u1 and u2 (≈ 1 line)\n",
    "    mu = None\n",
    "\n",
    "    # Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)\n",
    "    mu_B = None\n",
    "    mu_orth = None\n",
    "\n",
    "    # Step 4: Set u1_orth and u2_orth to be equal to mu_orth (≈2 lines)\n",
    "    u1_orth = None\n",
    "    u2_orth = None\n",
    "        \n",
    "    # Step 5: Adjust the Bias part of u1 and u2 using the formulas given in the figure above (≈2 lines)\n",
    "    u1_B = None\n",
    "    u2_B = None\n",
    "\n",
    "    # Step 6: Debias by equalizing u1 and u2 to the sum of their projections (≈2 lines)\n",
    "    u1 = None\n",
    "    u2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return u1, u2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"], gender))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"], gender))\n",
    "\n",
    "print()\n",
    "u1, u2 = equalize((\"man\", \"woman\"), gender)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(u1, gender) = \", cosine_similarity(u1, gender))\n",
    "print(\"cosine_similarity(u2, gender) = \", cosine_similarity(u2, gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "cosine similarities before equalizing:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(word_to_vec_map[\"man\"], gender)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         -0.117110957653\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(word_to_vec_map[\"woman\"], gender)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.356666188463\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "cosine similarities after equalizing:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(u1, gender)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         -0.700436428931\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(u2, gender)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.700436428931\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Please feel free to play with the above cell to equalize your own pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also encourage you to run your implementations to tackle other types of bias such as:\n",
    "- quantity, which can be encoded using: \"numerous\" - \"single\"\n",
    "- reality, which can be encoded using: \"real\" - \"fake\"\n",
    "- wealth, which can be encoded using: \"poor\" - \"rich\"\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "Congratulations on finishing this assignment. Here are the main points we would like you to remember:\n",
    "\n",
    "- Many operations such as cosine similarity or analogies can be applied to word vectors.\n",
    "- Cosine similarity is the main metric to compare word vectors, although L2 distance may also be used.\n",
    "- Your word vectors are learned by training a model on a dataset, they thus suffer from a bias inherent to the dataset.\n",
    "- There are different debiasing methods given some words are bias-specific (need to be equalized) while others are non-bias-specific (need to be neutralized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "- Bolukbasi et al., 2016, [Man is to Computer Programmer as Woman is to\n",
    "Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
